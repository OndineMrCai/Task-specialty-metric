{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02315e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import time\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4e2f2",
   "metadata": {},
   "source": [
    "## Define the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28528616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_cov(input_vec): \n",
    "    \"\"\"compute the covariance matrix(vector-horizontal, X.T*X)\"\"\"   \n",
    "    x = input_vec- torch.mean(input_vec,axis = 0)\n",
    "    cov_matrix = torch.mm(x.T, x) / x.shape[0] \n",
    "    return cov_matrix\n",
    "\n",
    "\n",
    "def compute_metric(hiddenstate_dict, device):\n",
    "    \"\"\"\"compute task-specialty metric in the paper\"\"\"\n",
    "    Groupmean_matrix = torch.cat([torch.mean(matrix.to(device), dim = 0,keepdim = True) for matrix in hiddenstate_dict.values()], dim = 0) # compute hy(l)\n",
    "    Betweenclass_matrix = torch_cov(Groupmean_matrix) # compute between-class variability\n",
    "    Withinclass_matirx = torch.zeros(Betweenclass_matrix.size()).to(device) \n",
    "    for matrix in hiddenstate_dict.values():\n",
    "        Withinclass_matirx = Withinclass_matirx + torch_cov(matrix.to(device)) / len(hiddenstate_dict) #compute within-class variability\n",
    "        del matrix \n",
    "    metric = torch.trace(torch.mm(Withinclass_matirx, torch.linalg.pinv(Betweenclass_matrix))) / len(hiddenstate_dict) #compute the metirc\n",
    "    del Withinclass_matirx, Betweenclass_matrix\n",
    "    for vector in hiddenstate_vectors.values():\n",
    "        del vector\n",
    "    torch.cuda.empty_cache()\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e54de4",
   "metadata": {},
   "source": [
    "## Load the tokenizer and Pretrained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e1c90ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/csc/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9 were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large', output_hidden_states= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12a230",
   "metadata": {},
   "source": [
    "## Load the data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f99be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "with open('data/train.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip('\\n').split('\\t')  \n",
    "        train_data = train_data + [line[3]]\n",
    "        train_label = train_label + [line [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc26caf",
   "metadata": {},
   "source": [
    "## Get the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f83828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_empty_dict():\n",
    "    \"\"\"create a list of 25 empty dict\"\"\"\n",
    "    All_layer_dict = []\n",
    "    for i in range(25):\n",
    "        All_layer_dict = All_layer_dict + [{'0':'','1':''}]  # every single layer has one dict \n",
    "    return All_layer_dict\n",
    "\n",
    "def hidden_state(train_data, train_label, All_layer_dict, device):\n",
    "    \"\"\"get the hidden state and store into the list\"\"\"\n",
    "    for i in range(len(train_data)):\n",
    "        encoded_input = tokenizer(train_data[i], return_tensors='pt')  #tokenize the sequence\n",
    "        with torch.no_grad():  # cancel gradient\n",
    "            output = model(**encoded_input)\n",
    "        if train_label[i] == '0':\n",
    "            if All_layer_dict[0]['0'] == '':\n",
    "                for j in range(len(output.hidden_states)):\n",
    "                    All_layer_dict[j]['0'] = torch.mean(output.hidden_states[j],axis=1)  # mean of every sequence\n",
    "            else:\n",
    "                for j in range(len(output.hidden_states)):\n",
    "                    All_layer_dict[j]['0'] = torch.cat((All_layer_dict[j]['0'],torch.mean(output.hidden_states[j],axis=1)),dim = 0)\n",
    "        elif train_label[i] == '1':\n",
    "            if All_layer_dict[0]['1'] == '':\n",
    "                for j in range(len(output.hidden_states)):\n",
    "                    All_layer_dict[j]['1'] = torch.mean(output.hidden_states[j],axis=1)\n",
    "            else:\n",
    "                for j in range(len(output.hidden_states)):\n",
    "                    All_layer_dict[j]['1'] = torch.cat((All_layer_dict[j]['1'],torch.mean(output.hidden_states[j],axis=1)),dim = 0)\n",
    "    print('done')\n",
    "    return All_layer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364acc98",
   "metadata": {},
   "source": [
    "## Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1429a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOdata = len(train_data)\n",
    "m_list = []         \n",
    "All_layer_dict = creat_empty_dict() \n",
    "All_layer_dict = hidden_state(train_data, train_label, All_layer_dict, device) \n",
    "for j in range(len(All_layer_dict)):\n",
    "    m = compute_metric(All_layer_dict[j], device).cpu() # compute every layer's metric in GPU\n",
    "    print(m)\n",
    "    m_list = m_list + [m] \n",
    "    del All_layer_dict[j]['0']\n",
    "    del All_layer_dict[j]['1']\n",
    "    torch.cuda.empty_cache() # release the GPU cache\n",
    "\n",
    "# plot the target figure\n",
    "x = list(range(0,25))\n",
    "plt.figure()\n",
    "plt.plot(x,m_list)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
